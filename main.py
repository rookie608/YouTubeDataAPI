# -*- coding: utf-8 -*-
"""
YouTube Data API v3 ã§:
- ãƒˆã‚¤ãƒ¬ãƒ»ä½å®…é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¤œç´¢ï¼ˆORæ¡ä»¶ï¼‰
- ç™»éŒ²è€… 9,000ã€œ300,000
- ç›´è¿‘6ãƒ¶æœˆä»¥å†…ã«æ›´æ–°ã‚ã‚Š
- å¤§æ‰‹ãƒ¡ãƒ‡ã‚£ã‚¢/ä¼æ¥­/èŠ¸èƒ½ç³»ã‚’é™¤å¤–ï¼ˆç·©ã‚ï¼‰
- CSVå‡ºåŠ›:
   ãƒãƒ£ãƒ³ãƒãƒ«å / ãƒãƒ³ãƒ‰ãƒ« / ãƒãƒ£ãƒ³ãƒãƒ«URL / ãƒãƒ³ãƒ‰ãƒ«URL / ç™»éŒ²è€…æ•° / æœ€çµ‚æŠ•ç¨¿æ—¥ / å‹•ç”»æœ¬æ•°
   ï¼ˆå®Ÿè¡Œæ—¥æ™‚å…¥ã‚Šãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
"""
import os
import sys
import time
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
import csv
import re
import requests

API_KEY = os.environ.get("YOUTUBE_API_KEY")
BASE = "https://www.googleapis.com/youtube/v3"

# ======== æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆORæ¡ä»¶ï¼‰ ========
KEYWORDS = [
    "ãƒˆã‚¤ãƒ¬", "ãƒˆã‚¤ãƒ¬ ãƒªãƒ•ã‚©ãƒ¼ãƒ ", "æ°´å›ã‚Š ãƒªãƒ•ã‚©ãƒ¼ãƒ ", "æ´—é¢æ‰€ ãƒªãƒ•ã‚©ãƒ¼ãƒ ",
    "ä½å®…", "ä½å®… ãƒªãƒ•ã‚©ãƒ¼ãƒ ", "é–“å–ã‚Š", "å†…è£… DIY", "ãƒªãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³",
    "ãƒˆã‚¤ãƒ¬ DIY", "ãƒã‚¹ãƒ«ãƒ¼ãƒ  ãƒªãƒ•ã‚©ãƒ¼ãƒ ", "ã‚­ãƒƒãƒãƒ³ ãƒªãƒ•ã‚©ãƒ¼ãƒ ", "ä¸­å¤ä½å®… ãƒªãƒ•ã‚©ãƒ¼ãƒ ",
]

REGION_CODE = "JP"
MAX_PAGES_PER_KEYWORD = 1

MIN_SUBSCRIBERS = 9_000
MAX_SUBSCRIBERS = 300_000
LATEST_WITHIN_DAYS = 183
STOP_AFTER_N_RESULTS = 200

# ======== é™¤å¤–æ¡ä»¶ï¼ˆç·©ã‚ï¼‰ ========
EXCLUDE_BRANDY_CHANNELS = True
EXCLUDE_NAME_PATTERNS = [
    r"NHK", r"æ—¥çµŒ", r"æœæ—¥", r"æ¯æ—¥", r"èª­å£²", r"ç”£çµŒ",
    r"ãƒ†ãƒ¬ãƒ“", r"TV",
    r"Panasonic", r"TOTO", r"LIXIL", r"YKK", r"ã‚¿ã‚«ãƒ©ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰",
    r"SUUMO", r"ãƒªã‚¯ã‚·ãƒ«", r"HOMES?",
    r"èŠ¸èƒ½|æœ‰åäºº|ã‚¢ã‚¤ãƒ‰ãƒ«",
]
# ==============================================


def yt_get(path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """å…±é€šGETï¼ˆãƒªãƒˆãƒ©ã‚¤ã¤ããƒ»ä¾‹å¤–æ•æ‰ï¼‰"""
    import random
    params = {**params, "key": API_KEY}
    last_resp = None
    last_exc = None
    for i in range(4):
        try:
            resp = requests.get(f"{BASE}/{path}", params=params, timeout=(5, 40))
            if resp.status_code == 200:
                return resp.json()
            last_resp = resp
        except requests.exceptions.RequestException as e:
            last_exc = repr(e)
        time.sleep((2 ** i) + random.uniform(0, 0.5))
    safe_params = {k: v for k, v in params.items() if k != "key"}
    return {
        "__error__": True,
        "__http_status": (last_resp.status_code if last_resp else None),
        "__url": (last_resp.url if last_resp else f"{BASE}/{path}"),
        "__text": (last_resp.text[:500] if last_resp else None),
        "__exc": last_exc,
        "__params": safe_params,
    }


def warn(tag: str, resp: Dict[str, Any]):
    print(
        f"[WARN] {tag} status={resp.get('__http_status')} url={resp.get('__url')}\n"
        f"       exc={resp.get('__exc')}\n"
        f"       body={resp.get('__text')}\n"
        f"       params={resp.get('__params')}\n",
        file=sys.stderr
    )


def sanity_check() -> bool:
    """APIã‚­ãƒ¼/ã‚¯ã‚©ãƒ¼ã‚¿ç–é€šç¢ºèª"""
    test_id = "UC_x5XG1OV2P6uZZ5FSM9Ttw"  # Google Developers
    resp = yt_get("channels", {"part": "id", "id": test_id})
    if resp.get("__error__"):
        print("[SANITY] FAILED", file=sys.stderr)
        warn("channels.list (sanity)", resp)
        return False
    print("[SANITY] OK")
    return True


def search_channels(keyword: str, max_pages: int = 1, region_code: Optional[str] = "JP") -> List[str]:
    """search.list ã§ channelId ã‚’åé›†"""
    ids: set[str] = set()
    page_token = None
    for _ in range(max_pages):
        resp = yt_get("search", {
            "part": "snippet",
            "q": keyword,
            "type": "channel",
            "maxResults": 50,
            "regionCode": region_code,
            "relevanceLanguage": "ja",
            "pageToken": page_token
        })
        if resp.get("__error__"):
            warn("search.list", resp)
            break
        for it in resp.get("items", []):
            snip = it.get("snippet") or {}
            cid = snip.get("channelId")
            if cid:
                ids.add(cid)
        page_token = resp.get("nextPageToken")
        if not page_token:
            break
    return list(ids)


def search_channels_multi(keywords: List[str], max_pages: int, region_code: str) -> List[str]:
    """è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ OR æ¡ä»¶ã§çµåˆ"""
    all_ids: set[str] = set()
    for kw in keywords:
        print(f"ğŸ” Searching for keyword: {kw}")
        all_ids.update(search_channels(kw, max_pages=max_pages, region_code=region_code))
        if len(all_ids) >= STOP_AFTER_N_RESULTS * 3:
            break
    return list(all_ids)


def chunk(lst: List[str], n: int) -> List[List[str]]:
    return [lst[i:i+n] for i in range(0, len(lst), n)]


def get_channels_details(channel_ids: List[str]) -> List[Dict[str, Any]]:
    """channels.list ã§è©³ç´°å–å¾—"""
    results: List[Dict[str, Any]] = []
    for batch in chunk(channel_ids, 50):
        resp = yt_get("channels", {
            "part": "statistics,snippet,contentDetails",
            "id": ",".join(batch)
        })
        if resp.get("__error__"):
            warn("channels.list", resp)
            continue
        results += resp.get("items", [])
    return results


def get_latest_upload_published_at(uploads_playlist_id: str) -> Optional[str]:
    """æœ€æ–°å‹•ç”»ã®å…¬é–‹æ—¥å–å¾—"""
    if not uploads_playlist_id:
        return None
    resp = yt_get("playlistItems", {
        "part": "snippet,contentDetails",
        "playlistId": uploads_playlist_id,
        "maxResults": 1
    })
    if resp.get("__error__"):
        warn("playlistItems.list", resp)
        return None
    items = resp.get("items", [])
    if not items:
        return None
    return items[0]["snippet"].get("publishedAt")


def iso_to_dt(iso: Optional[str]) -> Optional[datetime]:
    if not iso:
        return None
    return datetime.fromisoformat(iso.replace("Z", "+00:00")).astimezone(timezone.utc)


def any_match(patterns: List[str], text: str) -> bool:
    if not text:
        return False
    for p in patterns:
        if re.search(p, text, flags=re.IGNORECASE):
            return True
    return False


def save_csv(rows: List[Dict[str, Any]]) -> str:
    """CSVå‡ºåŠ›ï¼ˆutf-8-sigï¼‰"""
    now_str = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"output_youtube_channels_{now_str}.csv"
    out_path = os.path.abspath(filename)
    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=[
            "ãƒãƒ£ãƒ³ãƒãƒ«å", "ãƒãƒ³ãƒ‰ãƒ«", "YouTubeã®URL", "ãƒãƒ³ãƒ‰ãƒ«URL",
            "ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰", "æœ€çµ‚æŠ•ç¨¿æ—¥", "å‹•ç”»æœ¬æ•°",
            "channel_id", "channel_started_at", "description"
        ])
        w.writeheader()
        for r in rows:
            w.writerow(r)
    print(f"\nâœ… CSVã«ä¿å­˜ã—ã¾ã—ãŸ â†’ {out_path}")
    return out_path


def main():
    if not API_KEY:
        print("ç’°å¢ƒå¤‰æ•° YOUTUBE_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚", file=sys.stderr)
        sys.exit(1)

    if not sanity_check():
        print("ã‚¯ã‚©ãƒ¼ã‚¿è¶…é/ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/ã‚­ãƒ¼åˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
        sys.exit(2)

    latest_after_dt = datetime.now(timezone.utc) - timedelta(days=LATEST_WITHIN_DAYS)

    # 1ï¸âƒ£ ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢ï¼ˆORæ¡ä»¶ï¼‰
    channel_ids = search_channels_multi(KEYWORDS, max_pages=MAX_PAGES_PER_KEYWORD, region_code=REGION_CODE)
    if not channel_ids:
        print("è©²å½“ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # 2ï¸âƒ£ è©³ç´°å–å¾—
    details = get_channels_details(channel_ids)

    # 3ï¸âƒ£ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    results = []
    for ch in details:
        stat = ch.get("statistics", {}) or {}
        snip = ch.get("snippet", {}) or {}
        cdet = ch.get("contentDetails", {}) or {}
        uploads_id = (cdet.get("relatedPlaylists") or {}).get("uploads", "")
        channel_id = ch["id"]
        title = snip.get("title") or ""
        desc = snip.get("description") or ""
        handle = snip.get("customUrl") or ""  # ä¾‹: @toivo6583
        handle_url = f"https://www.youtube.com/{handle}" if handle else ""

        # é™¤å¤–
        if EXCLUDE_BRANDY_CHANNELS:
            if any_match(EXCLUDE_NAME_PATTERNS, f"{title} {desc}"):
                continue

        sub_raw = stat.get("subscriberCount")
        video_count_raw = stat.get("videoCount")
        if sub_raw is None:
            continue
        subscribers = int(sub_raw)
        video_count = int(video_count_raw) if video_count_raw else None

        if subscribers < MIN_SUBSCRIBERS or subscribers > MAX_SUBSCRIBERS:
            continue

        latest_iso = get_latest_upload_published_at(uploads_id)
        latest_dt = iso_to_dt(latest_iso) if latest_iso else None
        if not latest_dt or latest_dt < latest_after_dt:
            continue

        channel_url = f"https://www.youtube.com/channel/{channel_id}"
        results.append({
            "ãƒãƒ£ãƒ³ãƒãƒ«å": title,
            "ãƒãƒ³ãƒ‰ãƒ«": handle,
            "YouTubeã®URL": channel_url,
            "ãƒãƒ³ãƒ‰ãƒ«URL": handle_url,
            "ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰": subscribers,
            "æœ€çµ‚æŠ•ç¨¿æ—¥": latest_iso,
            "å‹•ç”»æœ¬æ•°": video_count,
            "channel_id": channel_id,
            "channel_started_at": snip.get("publishedAt"),
            "description": desc
        })

    # 4ï¸âƒ£ å‡ºåŠ›
    if not results:
        print("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    results = sorted(results, key=lambda x: (x["ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰"], x["ãƒãƒ£ãƒ³ãƒãƒ«å"] or ""))
    print("ãƒãƒ£ãƒ³ãƒãƒ«å,ãƒãƒ³ãƒ‰ãƒ«,YouTubeã®URL,ãƒãƒ³ãƒ‰ãƒ«URL,ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰,æœ€çµ‚æŠ•ç¨¿æ—¥,å‹•ç”»æœ¬æ•°")
    for r in results:
        print(f'{r["ãƒãƒ£ãƒ³ãƒãƒ«å"]},{r["ãƒãƒ³ãƒ‰ãƒ«"]},{r["YouTubeã®URL"]},{r["ãƒãƒ³ãƒ‰ãƒ«URL"]},{r["ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰"]},{r["æœ€çµ‚æŠ•ç¨¿æ—¥"]},{r["å‹•ç”»æœ¬æ•°"]}')

    save_csv(results)


if __name__ == "__main__":
    main()

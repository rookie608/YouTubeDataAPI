# main_deck-brush.py
# -*- coding: utf-8 -*-
"""
YouTube Data API v3 ã§:
- æ¸…æƒé–¢é€£ï¼ˆãƒã‚¦ã‚¹ã‚­ãƒ¼ãƒ‘ãƒ¼/å®¶äº‹ä»£è¡Œ/å±‹å†…æ¸…æƒãªã©ï¼‰ã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ OR æ¡ä»¶ã§åé›†
- ç™»éŒ²è€… 9,000ã€œ300,000 ã‚’é€šã™ï¼ˆç´„9,000äººä»¥ä¸Š ã€œ 30ä¸‡äººä»¥ä¸‹ï¼‰
- ç›´è¿‘6ãƒ¶æœˆä»¥å†…ã«æ›´æ–°ãŒã‚ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ã®ã¿
- å¤§æ‰‹ä¼æ¥­/å”ä¼š/ãƒ¡ãƒ‡ã‚£ã‚¢/æœ‰åäººã£ã½ã„ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ç·©ã‚ã«é™¤å¤–
- CSVå‡ºåŠ›: å…ˆé ­åˆ— = ãƒãƒ£ãƒ³ãƒãƒ«å / ãƒãƒ³ãƒ‰ãƒ« / ãƒãƒ£ãƒ³ãƒãƒ«URL / ãƒãƒ³ãƒ‰ãƒ«URL / ç™»éŒ²è€…æ•° / æœ€çµ‚æŠ•ç¨¿æ—¥ / å‹•ç”»æœ¬æ•°
- å®Ÿè¡Œæ—¥æ™‚å…¥ã‚Šãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆYYYYMMDD_HHMMï¼‰

ä½¿ã„æ–¹:
  export YOUTUBE_API_KEY="ã‚ãªãŸã®APIã‚­ãƒ¼"
  python main_deck-brush.py

â€» ã‚¯ã‚©ãƒ¼ã‚¿ç¯€ç´„: search.list ã¯å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1ãƒšãƒ¼ã‚¸ã®ã¿ï¼ˆ100ãƒ¦ãƒ‹ãƒƒãƒˆ/å›ï¼‰
"""

import os
import sys
import time
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
import csv
import re
import requests

API_KEY = os.environ.get("YOUTUBE_API_KEY")
BASE = "https://www.googleapis.com/youtube/v3"

# ======== æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ¸…æƒç³»ãƒ»ORæ¡ä»¶ï¼‰ ========
KEYWORDS = [
    "æ¸…æƒ", "æƒé™¤", "ãƒã‚¦ã‚¹ã‚­ãƒ¼ãƒ‘ãƒ¼", "å®¶äº‹ä»£è¡Œ", "ãƒã‚¦ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°",
    "å±‹å†… æ¸…æƒ", "ãƒ«ãƒ¼ãƒ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°", "ç‰‡ä»˜ã‘", "æ•´ç†åç´",
    "æ°´å›ã‚Š æƒé™¤", "ãƒˆã‚¤ãƒ¬ æƒé™¤", "ãƒã‚¹ãƒ«ãƒ¼ãƒ  æƒé™¤", "ã‚­ãƒƒãƒãƒ³ æƒé™¤",
    "æ›æ°—æ‰‡ æƒé™¤", "ã‚¨ã‚¢ã‚³ãƒ³ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°", "åºŠ ãƒ¯ãƒƒã‚¯ã‚¹ æƒé™¤", "ã‚«ãƒ“å–ã‚Š æƒé™¤",
]

REGION_CODE = "JP"
MAX_PAGES_PER_KEYWORD = 1          # ã‚¯ã‚©ãƒ¼ã‚¿ç¯€ç´„
STOP_AFTER_N_RESULTS = 200         # ä¿é™ºã§ä¸Šé™

# ======== æ•°å€¤æ¡ä»¶ ========
MIN_SUBSCRIBERS = 9_000
MAX_SUBSCRIBERS = 300_000
LATEST_WITHIN_DAYS = 183           # ç›´è¿‘6ãƒ¶æœˆä»¥å†…

# ======== é™¤å¤–ï¼ˆç·©ã‚ï¼šå¤§æ‰‹ãƒ¡ãƒ‡ã‚£ã‚¢/èŠ¸èƒ½/å¤§ä¼æ¥­ã£ã½ã„ï¼‰ ========
EXCLUDE_BRANDY_CHANNELS = True
EXCLUDE_NAME_PATTERNS = [
    # å¤§æ‰‹ãƒ¡ãƒ‡ã‚£ã‚¢
    r"NHK", r"æ—¥çµŒ", r"æœæ—¥", r"æ¯æ—¥", r"èª­å£²", r"ç”£çµŒ",
    r"ãƒ†ãƒ¬ãƒ“", r"TV",
    # æ¸…æƒå¤§æ‰‹ç³»ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆé™¤å¤–ã—ãŸã„å ´åˆï¼‰
    r"ãƒ€ã‚¹ã‚­ãƒ³", r"ãŠãã†ã˜æœ¬èˆ—", r"ãƒ™ã‚¢ãƒ¼ã‚º", r"CaSy", r"ãƒ‹ãƒã‚¤",
    # èŠ¸èƒ½ç³»
    r"èŠ¸èƒ½|æœ‰åäºº|ã‚¢ã‚¤ãƒ‰ãƒ«",
]

# ================== HTTPãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==================
def yt_get(path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """å…±é€šGETï¼ˆä¾‹å¤–æ•æ‰ãƒ»æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‹ã‚¸ãƒƒã‚¿ãƒ¼ï¼‰"""
    import random
    params = {**params, "key": API_KEY}
    last_resp = None
    last_exc = None
    for i in range(4):
        try:
            resp = requests.get(f"{BASE}/{path}", params=params, timeout=(5, 40))
            if resp.status_code == 200:
                return resp.json()
            last_resp = resp
        except requests.exceptions.RequestException as e:
            last_exc = repr(e)
        time.sleep((2 ** i) + random.uniform(0, 0.5))
    safe_params = {k: v for k, v in params.items() if k != "key"}
    return {
        "__error__": True,
        "__http_status": (last_resp.status_code if last_resp else None),
        "__url": (last_resp.url if last_resp else f"{BASE}/{path}"),
        "__text": (last_resp.text[:500] if last_resp else None),
        "__exc": last_exc,
        "__params": safe_params,
    }

def warn(tag: str, resp: Dict[str, Any]):
    print(
        f"[WARN] {tag} status={resp.get('__http_status')} url={resp.get('__url')}\n"
        f"       exc={resp.get('__exc')}\n"
        f"       body={resp.get('__text')}\n"
        f"       params={resp.get('__params')}\n",
        file=sys.stderr
    )

def sanity_check() -> bool:
    """APIã‚­ãƒ¼/ã‚¯ã‚©ãƒ¼ã‚¿ç–é€šç¢ºèªï¼ˆ1ãƒ¦ãƒ‹ãƒƒãƒˆï¼‰"""
    test_id = "UC_x5XG1OV2P6uZZ5FSM9Ttw"  # Google Developers
    resp = yt_get("channels", {"part": "id", "id": test_id})
    if resp.get("__error__"):
        print("[SANITY] FAILED", file=sys.stderr)
        warn("channels.list (sanity)", resp)
        return False
    print("[SANITY] OK")
    return True

# ================== APIãƒ©ãƒƒãƒ‘ ==================
def search_channels(keyword: str, max_pages: int = 1, region_code: Optional[str] = "JP") -> List[str]:
    """search.list ã§ channelId ã‚’åé›†ï¼ˆ1å›=100ãƒ¦ãƒ‹ãƒƒãƒˆï¼‰"""
    ids: set[str] = set()
    page_token = None
    for _ in range(max_pages):
        resp = yt_get("search", {
            "part": "snippet",
            "q": keyword,
            "type": "channel",
            "maxResults": 50,
            "regionCode": region_code,
            "relevanceLanguage": "ja",
            "pageToken": page_token
        })
        if resp.get("__error__"):
            warn("search.list", resp)
            break
        for it in resp.get("items", []):
            snip = it.get("snippet") or {}
            cid = snip.get("channelId")
            if cid:
                ids.add(cid)
        page_token = resp.get("nextPageToken")
        if not page_token:
            break
    return list(ids)

def search_channels_multi(keywords: List[str], max_pages: int, region_code: str) -> List[str]:
    """è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ OR æ¡ä»¶ã§çµåˆ"""
    all_ids: set[str] = set()
    for kw in keywords:
        print(f"ğŸ” Searching: {kw}")
        all_ids.update(search_channels(kw, max_pages=max_pages, region_code=region_code))
        if len(all_ids) >= STOP_AFTER_N_RESULTS * 3:
            break
    return list(all_ids)

def chunk(lst: List[str], n: int) -> List[List[str]]:
    return [lst[i:i+n] for i in range(0, len(lst), n)]

def get_channels_details(channel_ids: List[str]) -> List[Dict[str, Any]]:
    """channels.list ã§è©³ç´°å–å¾—ï¼ˆstatistics,snippet,contentDetailsï¼‰"""
    results: List[Dict[str, Any]] = []
    for batch in chunk(channel_ids, 50):
        resp = yt_get("channels", {
            "part": "statistics,snippet,contentDetails",
            "id": ",".join(batch)
        })
        if resp.get("__error__"):
            warn("channels.list", resp)
            continue
        results += resp.get("items", [])
    return results

def get_latest_upload_published_at(uploads_playlist_id: str) -> Optional[str]:
    """æœ€æ–°å‹•ç”»ã®å…¬é–‹æ—¥ï¼ˆplaylistItems ã§1ä»¶ï¼‰"""
    if not uploads_playlist_id:
        return None
    resp = yt_get("playlistItems", {
        "part": "snippet,contentDetails",
        "playlistId": uploads_playlist_id,
        "maxResults": 1
    })
    if resp.get("__error__"):
        warn("playlistItems.list", resp)
        return None
    items = resp.get("items", [])
    if not items:
        return None
    return items[0]["snippet"].get("publishedAt")

# ================== ãƒ˜ãƒ«ãƒ‘ ==================
def iso_to_dt(iso: Optional[str]) -> Optional[datetime]:
    if not iso:
        return None
    return datetime.fromisoformat(iso.replace("Z", "+00:00")).astimezone(timezone.utc)

def any_match(patterns: List[str], text: str) -> bool:
    if not text:
        return False
    for p in patterns:
        if re.search(p, text, flags=re.IGNORECASE):
            return True
    return False

def save_csv(rows: List[Dict[str, Any]]) -> str:
    """CSVå‡ºåŠ›ï¼ˆutf-8-sigã§Exceläº’æ›ã€å®Ÿè¡Œæ—¥æ™‚å…¥ã‚Šãƒ•ã‚¡ã‚¤ãƒ«åï¼‰"""
    now_str = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"output_youtube_channels_{now_str}.csv"
    out_path = os.path.abspath(filename)
    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=[
            "ãƒãƒ£ãƒ³ãƒãƒ«å", "ãƒãƒ³ãƒ‰ãƒ«", "YouTubeã®URL", "ãƒãƒ³ãƒ‰ãƒ«URL",
            "ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰", "æœ€çµ‚æŠ•ç¨¿æ—¥", "å‹•ç”»æœ¬æ•°",
            "channel_id", "channel_started_at", "description"
        ])
        w.writeheader()
        for r in rows:
            w.writerow(r)
    print(f"\nâœ… CSVã«ä¿å­˜ã—ã¾ã—ãŸ â†’ {out_path}")
    return out_path

# ================== ãƒ¡ã‚¤ãƒ³ ==================
def main():
    if not API_KEY:
        print("ç’°å¢ƒå¤‰æ•° YOUTUBE_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚", file=sys.stderr)
        sys.exit(1)

    if not sanity_check():
        print("ã‚¯ã‚©ãƒ¼ã‚¿è¶…é/ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/ã‚­ãƒ¼åˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
        sys.exit(2)

    latest_after_dt = datetime.now(timezone.utc) - timedelta(days=LATEST_WITHIN_DAYS)

    # 1) å€™è£œåé›†ï¼ˆORæ¡ä»¶ï¼‰
    channel_ids = search_channels_multi(KEYWORDS, max_pages=MAX_PAGES_PER_KEYWORD, region_code=REGION_CODE)
    if not channel_ids:
        print("è©²å½“ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # 2) è©³ç´°å–å¾—
    details = get_channels_details(channel_ids)

    # 3) æ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿
    results = []
    for ch in details:
        stat = ch.get("statistics", {}) or {}
        snip = ch.get("snippet", {}) or {}
        cdet = ch.get("contentDetails", {}) or {}

        uploads_id = (cdet.get("relatedPlaylists") or {}).get("uploads", "")
        channel_id = ch.get("id")
        title = snip.get("title") or ""
        desc = snip.get("description") or ""
        handle = snip.get("customUrl") or ""                   # ä¾‹: @xxxxxx
        handle_url = f"https://www.youtube.com/{handle}" if handle else ""
        channel_url = f"https://www.youtube.com/channel/{channel_id}"

        # é™¤å¤–ï¼ˆç·©ã‚ï¼‰
        if EXCLUDE_BRANDY_CHANNELS:
            if any_match(EXCLUDE_NAME_PATTERNS, f"{title} {desc}"):
                continue

        # ç™»éŒ²è€…ãƒ»å‹•ç”»æœ¬æ•°
        sub_raw = stat.get("subscriberCount")
        video_count_raw = stat.get("videoCount")
        if sub_raw is None:
            continue
        subscribers = int(sub_raw)
        video_count = int(video_count_raw) if video_count_raw else None

        if subscribers < MIN_SUBSCRIBERS or subscribers > MAX_SUBSCRIBERS:
            continue

        # ç›´è¿‘æ›´æ–°ï¼ˆ6ãƒ¶æœˆä»¥å†…ï¼‰
        latest_iso = get_latest_upload_published_at(uploads_id)
        latest_dt = iso_to_dt(latest_iso) if latest_iso else None
        if not latest_dt or latest_dt < latest_after_dt:
            continue

        results.append({
            "ãƒãƒ£ãƒ³ãƒãƒ«å": title,
            "ãƒãƒ³ãƒ‰ãƒ«": handle,
            "YouTubeã®URL": channel_url,
            "ãƒãƒ³ãƒ‰ãƒ«URL": handle_url,
            "ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰": subscribers,
            "æœ€çµ‚æŠ•ç¨¿æ—¥": latest_iso,
            "å‹•ç”»æœ¬æ•°": video_count,
            "channel_id": channel_id,
            "channel_started_at": snip.get("publishedAt"),
            "description": desc
        })

        if len(results) >= STOP_AFTER_N_RESULTS:
            break

    # 4) å‡ºåŠ›
    if not results:
        print("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # é‡è¤‡é™¤å»ï¼†æ•´åˆ—
    uniq = {r["channel_id"]: r for r in results}
    results = sorted(uniq.values(), key=lambda x: (x["ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰"], x["ãƒãƒ£ãƒ³ãƒãƒ«å"] or ""))

    # ç”»é¢å‡ºåŠ›ï¼ˆç¢ºèªç”¨ï¼‰
    print("ãƒãƒ£ãƒ³ãƒãƒ«å,ãƒãƒ³ãƒ‰ãƒ«,YouTubeã®URL,ãƒãƒ³ãƒ‰ãƒ«URL,ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰,æœ€çµ‚æŠ•ç¨¿æ—¥,å‹•ç”»æœ¬æ•°")
    for r in results:
        t = (r["ãƒãƒ£ãƒ³ãƒãƒ«å"] or "").replace(",", "ï¼Œ")
        print(f'{t},{r["ãƒãƒ³ãƒ‰ãƒ«"]},{r["YouTubeã®URL"]},{r["ãƒãƒ³ãƒ‰ãƒ«URL"]},{r["ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰"]},{r["æœ€çµ‚æŠ•ç¨¿æ—¥"]},{r["å‹•ç”»æœ¬æ•°"]}')

    # CSVä¿å­˜
    save_csv(results)

if __name__ == "__main__":
    main()

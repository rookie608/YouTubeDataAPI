# main_deck-brush.py
# -*- coding: utf-8 -*-
"""
YouTube Data API v3 ã§:
- æ¸…æƒé–¢é€£ï¼ˆãƒã‚¦ã‚¹ã‚­ãƒ¼ãƒ‘ãƒ¼/å®¶äº‹ä»£è¡Œ/å±‹å†…æ¸…æƒãªã©ï¼‰ã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ OR æ¡ä»¶ã§åé›†
- ç™»éŒ²è€… 9,000ã€œ300,000 ã‚’é€šã™ï¼ˆç´„9,000äººä»¥ä¸Š ã€œ 30ä¸‡äººä»¥ä¸‹ï¼‰
- ç›´è¿‘6ãƒ¶æœˆä»¥å†…ã«æ›´æ–°ãŒã‚ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ã®ã¿
- å¤§æ‰‹ä¼æ¥­/å”ä¼š/ãƒ¡ãƒ‡ã‚£ã‚¢/æœ‰åäººã£ã½ã„ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ç·©ã‚ã«é™¤å¤–
- CSVå‡ºåŠ›: å…ˆé ­åˆ— = ãƒãƒ£ãƒ³ãƒãƒ«å / ãƒãƒ³ãƒ‰ãƒ« / ãƒãƒ£ãƒ³ãƒãƒ«URL / ãƒãƒ³ãƒ‰ãƒ«URL / ç™»éŒ²è€…æ•° / æœ€çµ‚æŠ•ç¨¿æ—¥ / å‹•ç”»æœ¬æ•°
- å®Ÿè¡Œæ—¥æ™‚å…¥ã‚Šãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆYYYYMMDD_HHMMï¼‰

ä½¿ã„æ–¹:
  export YOUTUBE_API_KEY="ã‚ãªãŸã®APIã‚­ãƒ¼"
  python main_deck-brush.py
"""

import os
import sys
import time
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
import csv
import re
import requests

API_KEY = os.environ.get("YOUTUBE_API_KEY")
BASE = "https://www.googleapis.com/youtube/v3"

# ======== æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ¸…æƒç³»ãƒ»ORæ¡ä»¶ï¼‰ ========
KEYWORDS = ["æ¸…æƒ", "æƒé™¤", "ãƒã‚¦ã‚¹ã‚­ãƒ¼ãƒ‘ãƒ¼", "å®¶äº‹ä»£è¡Œ", "ãƒã‚¦ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°", "å±‹å†…æ¸…æƒ", "ãƒ«ãƒ¼ãƒ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°", "ç‰‡ä»˜ã‘", "æ•´ç†åç´", "æ°´å›ã‚Šæƒé™¤", "ãƒˆã‚¤ãƒ¬æƒé™¤", "ãƒã‚¹ãƒ«ãƒ¼ãƒ æƒé™¤", "ã‚­ãƒƒãƒãƒ³æƒé™¤", "æ›æ°—æ‰‡æƒé™¤", "ã‚¨ã‚¢ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°", "åºŠãƒ¯ãƒƒã‚¯ã‚¹æƒé™¤", "ã‚«ãƒ“å–ã‚Šæƒé™¤", "ä¸»å©¦ã®ãƒãƒ£ãƒ³ãƒãƒ«", "ä¸»å©¦ãƒ©ã‚¤ãƒ•", "ä¸»å©¦ vlog", "å®¶äº‹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³", "æš®ã‚‰ã—ã®å·¥å¤«", "ç”Ÿæ´»ã®çŸ¥æµ", "å®¶äº‹ã®ã‚³ãƒ„", "æƒé™¤ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³", "æ™‚çŸ­å®¶äº‹", "å®¶äº‹ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³", "ãŠç‰‡ä»˜ã‘", "æ–­æ¨é›¢", "åç´ã‚¢ã‚¤ãƒ‡ã‚¢", "æ•´ç†è¡“", "ãƒŸãƒ‹ãƒãƒªã‚¹ãƒˆ", "ã‚·ãƒ³ãƒ—ãƒ«ãƒ©ã‚¤ãƒ•", "åç´ã‚°ãƒƒã‚º", "ç‰‡ä»˜ã‘è¡“", "åç´æ–¹æ³•", "ã‚¤ãƒ³ãƒ†ãƒªã‚¢æ•´ç†", "å¼•è¶Šã—æ¥­è€…", "å¼•è¶Šã—æº–å‚™", "å¼•è¶Šã—ã‚µãƒãƒ¼ãƒˆ", "ä¸ç”¨å“å›å", "è·é€ ã‚Š", "è·è§£ã", "è»¢å±…ã‚µãƒãƒ¼ãƒˆ", "å¼•è¶Šã—ç‰‡ä»˜ã‘", "å¼•è¶Šã—æ¸…æƒ", "å¼•è¶Šã—å¾Œæƒé™¤"]

REGION_CODE = "JP"
MAX_PAGES_PER_KEYWORD = 1          # ã‚¯ã‚©ãƒ¼ã‚¿ç¯€ç´„
STOP_AFTER_N_RESULTS = 200         # CSVå‡ºåŠ›ä¸Šé™ç›®å®‰

# ======== æ•°å€¤æ¡ä»¶ ========
MIN_SUBSCRIBERS = 9_000
MAX_SUBSCRIBERS = 300_000
LATEST_WITHIN_DAYS = 183           # ç›´è¿‘6ãƒ¶æœˆä»¥å†…

# ======== é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ ========
EXCLUDE_BRANDY_CHANNELS = True
EXCLUDE_NAME_PATTERNS = [
    r"NHK", r"æ—¥çµŒ", r"æœæ—¥", r"æ¯æ—¥", r"èª­å£²", r"ç”£çµŒ", r"ãƒ†ãƒ¬ãƒ“", r"TV",
    r"ãƒ€ã‚¹ã‚­ãƒ³", r"ãŠãã†ã˜æœ¬èˆ—", r"ãƒ™ã‚¢ãƒ¼ã‚º", r"CaSy", r"ãƒ‹ãƒã‚¤",
    r"èŠ¸èƒ½|æœ‰åäºº|ã‚¢ã‚¤ãƒ‰ãƒ«",
]

# ================== HTTPãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==================
def yt_get(path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    import random
    params = {**params, "key": API_KEY}
    last_resp = None
    last_exc = None
    for i in range(4):
        try:
            resp = requests.get(f"{BASE}/{path}", params=params, timeout=(5, 40))
            if resp.status_code == 200:
                return resp.json()
            last_resp = resp
        except requests.exceptions.RequestException as e:
            last_exc = repr(e)
        time.sleep((2 ** i) + random.uniform(0, 0.5))
    safe_params = {k: v for k, v in params.items() if k != "key"}
    return {
        "__error__": True,
        "__http_status": (last_resp.status_code if last_resp else None),
        "__url": (last_resp.url if last_resp else f"{BASE}/{path}"),
        "__text": (last_resp.text[:500] if last_resp else None),
        "__exc": last_exc,
        "__params": safe_params,
    }

def warn(tag: str, resp: Dict[str, Any]):
    print(
        f"[WARN] {tag} status={resp.get('__http_status')} url={resp.get('__url')}\n"
        f"       exc={resp.get('__exc')}\n"
        f"       body={resp.get('__text')}\n"
        f"       params={resp.get('__params')}\n",
        file=sys.stderr
    )

def sanity_check() -> bool:
    test_id = "UC_x5XG1OV2P6uZZ5FSM9Ttw"
    resp = yt_get("channels", {"part": "id", "id": test_id})
    if resp.get("__error__"):
        print("[SANITY] FAILED", file=sys.stderr)
        warn("channels.list (sanity)", resp)
        return False
    print("[SANITY] OK")
    return True

# ================== APIãƒ©ãƒƒãƒ‘ ==================
def search_channels(keyword: str, max_pages: int = 1, region_code: Optional[str] = "JP") -> List[str]:
    ids: set[str] = set()
    page_token = None
    for _ in range(max_pages):
        resp = yt_get("search", {
            "part": "snippet",
            "q": keyword,
            "type": "channel",
            "maxResults": 50,
            "regionCode": region_code,
            "relevanceLanguage": "ja",
            "pageToken": page_token
        })
        if resp.get("__error__"):
            warn("search.list", resp)
            break
        for it in resp.get("items", []):
            snip = it.get("snippet") or {}
            cid = snip.get("channelId")
            if cid:
                ids.add(cid)
        page_token = resp.get("nextPageToken")
        if not page_token:
            break
    return list(ids)

def search_channels_multi(keywords: List[str], max_pages: int, region_code: str) -> List[str]:
    """å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é †ç•ªã«æ¤œç´¢ï¼ˆé€”ä¸­ã§æ­¢ã‚ãšã€é€²æ—è¡¨ç¤ºä»˜ãï¼‰"""
    all_ids: set[str] = set()
    total = len(keywords)
    for idx, kw in enumerate(keywords, 1):
        print(f"ğŸ” Searching ({idx}/{total}): {kw}")
        got = search_channels(kw, max_pages=max_pages, region_code=region_code)
        before = len(all_ids)
        all_ids.update(got)
        added = len(all_ids) - before
        print(f"    â†³ fetched={len(got)} / unique_added={added} / unique_total={len(all_ids)}")
    return list(all_ids)

def chunk(lst: List[str], n: int) -> List[List[str]]:
    return [lst[i:i+n] for i in range(0, len(lst), n)]

def get_channels_details(channel_ids: List[str]) -> List[Dict[str, Any]]:
    results: List[Dict[str, Any]] = []
    for batch in chunk(channel_ids, 50):
        resp = yt_get("channels", {
            "part": "statistics,snippet,contentDetails",
            "id": ",".join(batch)
        })
        if resp.get("__error__"):
            warn("channels.list", resp)
            continue
        results += resp.get("items", [])
    return results

def get_latest_upload_published_at(uploads_playlist_id: str) -> Optional[str]:
    if not uploads_playlist_id:
        return None
    resp = yt_get("playlistItems", {
        "part": "snippet,contentDetails",
        "playlistId": uploads_playlist_id,
        "maxResults": 1
    })
    if resp.get("__error__"):
        warn("playlistItems.list", resp)
        return None
    items = resp.get("items", [])
    if not items:
        return None
    return items[0]["snippet"].get("publishedAt")

# ================== ãƒ˜ãƒ«ãƒ‘ ==================
def iso_to_dt(iso: Optional[str]) -> Optional[datetime]:
    if not iso:
        return None
    return datetime.fromisoformat(iso.replace("Z", "+00:00")).astimezone(timezone.utc)

def any_match(patterns: List[str], text: str) -> bool:
    if not text:
        return False
    for p in patterns:
        if re.search(p, text, flags=re.IGNORECASE):
            return True
    return False

def save_csv(rows: List[Dict[str, Any]]) -> str:
    now_str = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"output_youtube_channels_{now_str}.csv"
    out_path = os.path.abspath(filename)
    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=[
            "ãƒãƒ£ãƒ³ãƒãƒ«å", "ãƒãƒ³ãƒ‰ãƒ«", "YouTubeã®URL", "ãƒãƒ³ãƒ‰ãƒ«URL",
            "ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰", "æœ€çµ‚æŠ•ç¨¿æ—¥", "å‹•ç”»æœ¬æ•°",
            "channel_id", "channel_started_at", "description"
        ])
        w.writeheader()
        for r in rows:
            w.writerow(r)
    print(f"\nâœ… CSVã«ä¿å­˜ã—ã¾ã—ãŸ â†’ {out_path}")
    return out_path

# ================== ãƒ¡ã‚¤ãƒ³ ==================
def main():
    if not API_KEY:
        print("ç’°å¢ƒå¤‰æ•° YOUTUBE_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚", file=sys.stderr)
        sys.exit(1)

    if not sanity_check():
        print("ã‚¯ã‚©ãƒ¼ã‚¿è¶…é/ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/ã‚­ãƒ¼åˆ¶é™ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", file=sys.stderr)
        sys.exit(2)

    latest_after_dt = datetime.now(timezone.utc) - timedelta(days=LATEST_WITHIN_DAYS)

    # 1) å€™è£œåé›†
    channel_ids = search_channels_multi(KEYWORDS, max_pages=MAX_PAGES_PER_KEYWORD, region_code=REGION_CODE)
    if not channel_ids:
        print("è©²å½“ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # 2) è©³ç´°å–å¾—
    details = get_channels_details(channel_ids)

    # 3) ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    results = []
    for ch in details:
        stat = ch.get("statistics", {}) or {}
        snip = ch.get("snippet", {}) or {}
        cdet = ch.get("contentDetails", {}) or {}

        uploads_id = (cdet.get("relatedPlaylists") or {}).get("uploads", "")
        channel_id = ch.get("id")
        title = snip.get("title") or ""
        desc = snip.get("description") or ""
        handle = snip.get("customUrl") or ""
        handle_url = f"https://www.youtube.com/{handle}" if handle else ""
        channel_url = f"https://www.youtube.com/channel/{channel_id}"

        if EXCLUDE_BRANDY_CHANNELS and any_match(EXCLUDE_NAME_PATTERNS, f"{title} {desc}"):
            continue

        sub_raw = stat.get("subscriberCount")
        video_count_raw = stat.get("videoCount")
        if sub_raw is None:
            continue
        subscribers = int(sub_raw)
        video_count = int(video_count_raw) if video_count_raw else None

        if subscribers < MIN_SUBSCRIBERS or subscribers > MAX_SUBSCRIBERS:
            continue

        latest_iso = get_latest_upload_published_at(uploads_id)
        latest_dt = iso_to_dt(latest_iso) if latest_iso else None
        if not latest_dt or latest_dt < latest_after_dt:
            continue

        results.append({
            "ãƒãƒ£ãƒ³ãƒãƒ«å": title,
            "ãƒãƒ³ãƒ‰ãƒ«": handle,
            "YouTubeã®URL": channel_url,
            "ãƒãƒ³ãƒ‰ãƒ«URL": handle_url,
            "ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰": subscribers,
            "æœ€çµ‚æŠ•ç¨¿æ—¥": latest_iso,
            "å‹•ç”»æœ¬æ•°": video_count,
            "channel_id": channel_id,
            "channel_started_at": snip.get("publishedAt"),
            "description": desc
        })

        if len(results) >= STOP_AFTER_N_RESULTS:
            break

    if not results:
        print("æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    uniq = {r["channel_id"]: r for r in results}
    results = sorted(uniq.values(), key=lambda x: (x["ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰"], x["ãƒãƒ£ãƒ³ãƒãƒ«å"] or ""))

    print("ãƒãƒ£ãƒ³ãƒãƒ«å,ãƒãƒ³ãƒ‰ãƒ«,YouTubeã®URL,ãƒãƒ³ãƒ‰ãƒ«URL,ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰,æœ€çµ‚æŠ•ç¨¿æ—¥,å‹•ç”»æœ¬æ•°")
    for r in results:
        t = (r["ãƒãƒ£ãƒ³ãƒãƒ«å"] or "").replace(",", "ï¼Œ")
        print(f'{t},{r["ãƒãƒ³ãƒ‰ãƒ«"]},{r["YouTubeã®URL"]},{r["ãƒãƒ³ãƒ‰ãƒ«URL"]},{r["ç™»éŒ²è€…æ•°ï¼ˆäººï¼‰"]},{r["æœ€çµ‚æŠ•ç¨¿æ—¥"]},{r["å‹•ç”»æœ¬æ•°"]}')

    save_csv(results)

if __name__ == "__main__":
    main()
